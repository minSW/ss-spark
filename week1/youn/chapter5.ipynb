{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "thorough-recognition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "increased-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\") \\\n",
    "  .load(\"data/flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "severe-simon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df. printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "further-departure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of Schema on Read(data source defines a schema)\n",
    "\n",
    "spark.read.format(\"json\") \\\n",
    "  .load(\"data/flight-data/json/2015-summary.json\") \\\n",
    "  .schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "binary-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of reading data with schema we defined already\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
    "])\n",
    "\n",
    "df = spark.read.format(\"json\") \\\n",
    "  .schema(myManualSchema) \\\n",
    "  .load(\"data/flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-alexander",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns and Expressions\n",
    "\n",
    "# cannot manipulate an individual column outside the context of a DataFrame\n",
    "# must use Spark transformations to modify a column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "juvenile-ethics",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'col'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d7614b541895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Explicit column references\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.6.8/envs/ss-spark/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1305\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1306\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'col'"
     ]
    }
   ],
   "source": [
    "# How to make columns: col or column functions\n",
    "\n",
    "from pyspark.sql.functions import col, column\n",
    "\n",
    "col(\"someColumnName\")\n",
    "column(\"someColumnName\")\n",
    "\n",
    "# Columns are not resolved until we compare the column names \n",
    "# with those we are maintaining in the catalog\n",
    "\n",
    "# Explicit column references\n",
    "# useful for a join\n",
    "# analyzer 단계에서 resolve 하는 것을 생략한다.\n",
    "\n",
    "df.col(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-percentage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experssions\n",
    "# a set of transformations on one or more values in a record in a DataFrame.\n",
    "# It's like a function that takes as input one or more columns names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "australian-commonwealth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accessing a dataframe's columns\n",
    "spark.read.format(\"json\") \\\n",
    "  .load(\"data/flight-data/json/2015-summary.json\") \\\n",
    "  .columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row \n",
    "\n",
    "## Creating Rows \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "extraordinary-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|            Hello|               null|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a dataframe\n",
    "df = spark.read.format(\"json\") \\\n",
    "  .load(\"data/flight-data/json/2015-summary.json\")\n",
    "\n",
    "# register df to temp view\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "\n",
    "# create a dataframe with custom rows and schema\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
    "])\n",
    "myRow = Row(\"Hello\", None, 1)\n",
    "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "willing-representative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, DEST_COUNTRY_NAME: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select and selectExpr\n",
    "\n",
    "df.select(\"DEST_COUNTRY_NAME\").show(2)\n",
    "\n",
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)\n",
    "\n",
    "from pyspark.sql.functions import expr, col, column\n",
    "\n",
    "df.select(\n",
    "  expr(\"DEST_COUNTRY_NAME\"),\n",
    "  col(\"DEST_COUNTRY_NAME\"),\n",
    "  column(\"DEST_COUNTRY_NAME\"))\\\n",
    "  .show(2)\n",
    "\n",
    "# Error -> not Error\n",
    "df.select(col(\"DEST_COUNTRY_NAME\"), \"DEST_COUNTRY_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sharp-device",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+\n",
      "|    United States|    United States|\n",
      "|    United States|    United States|\n",
      "+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.select(F.col(\"DEST_COUNTRY_NAME\"), \"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "burning-butter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting to Spark Types(Literals)\n",
    "# a translation from a given programming language's literal value to one that Spark understands.\n",
    "# Literals are expressions.\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.select(F.expr(\"*\"), F.lit(1).alias(\"One\")).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caroline-mileage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count', 'Destination']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding Columns\n",
    "# withColumn method: adding a new column to a DataFrame.\n",
    "# withColumn takes two arguments: (column_name: string, expression: expression)\n",
    "\n",
    "\n",
    "df.withColumn(\"numberOne\", F.lit(1)).show(2)\n",
    "\n",
    "df.withColumn(\"withinCountry\", F.expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")) \\\n",
    "  .show(2)\n",
    "\n",
    "# Renaming a column by using withColumn\n",
    "df.withColumn(\"Destination\", F.expr(\"DEST_COUNTRY_NAME\")).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "productive-coral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dest', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renaming Columns\n",
    "# withColumnRenamed method\n",
    "# withColumnRenamed() takes two arguments: (existing: string, new: string)\n",
    "\n",
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "suffering-planner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n",
      "|This Long Column-Name|new col|\n",
      "+---------------------+-------+\n",
      "|              Romania|Romania|\n",
      "|              Croatia|Croatia|\n",
      "+---------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This Long Column-Name']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reserved Characters and Keywords\n",
    "# reserved characters: ' ', '-' etc\n",
    "# keywords: time, date etc\n",
    "# use backtick(`)\n",
    "\n",
    "dfWithLongColName = df.withColumn(\n",
    "    \"This Long Column-Name\",\n",
    "    F.expr(\"ORIGIN_COUNTRY_NAME\"))\n",
    "\n",
    "dfWithLongColName.selectExpr(\n",
    "    \"`This Long Column-Name`\",\n",
    "    \"`This Long Column-Name` as `new col`\") \\\n",
    "    .show(2)\n",
    "\n",
    "dfWithLongColName.select(F.expr(\"`This Long Column-Name`\")).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "international-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Sensitivity\n",
    "# default: case insensitivity\n",
    "\n",
    "# set spark.sql.caseSensitive true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "completed-stock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DEST_COUNTRY_NAME', 'count']\n",
      "['count', 'This Long Column-Name']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count', 'This Long Column-Name']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Columns\n",
    "# drop method takes multiple arguements: (*cols)\n",
    "\n",
    "print(df.drop(\"ORIGIN_COUNTRY_NAME\").columns)\n",
    "\n",
    "print(dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").columns)\n",
    "dfWithLongColName.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "environmental-claim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint, count2: bigint]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing a Column's Type(cast)\n",
    "\n",
    "df.withColumn(\"count2\", F.col(\"count\").cast(\"long\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "optimum-affiliate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering Rows\n",
    "\n",
    "df.filter(F.col(\"count\") < 2).show(2)\n",
    "df.where(\"count < 2\").show(2)\n",
    "\n",
    "df.where(F.col(\"count\") < 2).where(F.col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\") \\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "jewish-algorithm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting Unique Rows\n",
    "\n",
    "df.select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "level-settlement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Samples\n",
    "\n",
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.5\n",
    "\n",
    "df.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "verbal-matrix",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Splits\n",
    "\n",
    "seed = 5\n",
    "dfs = df.randomSplit([0.25, 0.75], seed)\n",
    "dfs[0].count() > dfs[1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "incorporate-conjunction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "|    United States|             Cyprus|    1|\n",
      "|    United States|            Estonia|    1|\n",
      "|    United States|          Lithuania|    1|\n",
      "|    United States|           Bulgaria|    1|\n",
      "|    United States|            Georgia|    1|\n",
      "|    United States|            Bahrain|    1|\n",
      "|    United States|   Papua New Guinea|    1|\n",
      "|    United States|         Montenegro|    1|\n",
      "|    United States|            Namibia|    1|\n",
      "|    New Country 2|    Other Country 3|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenating and Appending Rows(Union)\n",
    "from pyspark.sql import Row\n",
    "schema = df.schema\n",
    "newRows = [\n",
    "    Row(\"New Country\", \"Other Country\", 5),\n",
    "    Row(\"New Country 2\", \"Other Country 3\", 1)\n",
    "]\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)\n",
    "\n",
    "df.union(newDF) \\\n",
    "  .where(\"count = 1\") \\\n",
    "  .where(F.col(\"ORIGIN_COUNTRY_NAME\") != \"United States\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "expired-syria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sorting Rows\n",
    "# sort and orderBy are exactly same method\n",
    "# asc, desc, asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last\n",
    "# sortWithinPartitions\n",
    "\n",
    "\n",
    "df.sort(\"count\").show(5)\n",
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\n",
    "df.orderBy(F.col(\"count\"), F.col(\"DEST_COUNTRY_NAME\")).show(5)\n",
    "\n",
    "df.orderBy(F.expr(\"count desc\")).show(2)\n",
    "df.orderBy(F.col(\"count\").desc(), F.col(\"DEST_COUNTRY_NAME\").asc()).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "massive-representative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Limit\n",
    "\n",
    "df.limit(5).show()\n",
    "df.orderBy(F.expr(\"count desc\")).limit(6).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "optimum-browser",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repartition and Coalesce\n",
    "# Repartition will incur a full shuffle\n",
    "# 특정 column을 기준으로 자주 filtering 한다면, 그 column을 기준으로 repartition을 해라.\n",
    "# coalesce : 전체 파티션을 셔플하지 않고, 파티션을 병합하려는 경우에 사용.\n",
    "\n",
    "df.rdd.getNumPartitions()\n",
    "\n",
    "df.repartition(5)\n",
    "\n",
    "\n",
    "# 목적지를 기준으로 5개의 파티션으로 나누고, 셔플 없이 2개의 파티션으로 병합\n",
    "df.repartition(5, F.col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "affecting-version",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'false'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-c0c4017b3343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcollectDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcollectDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcollectDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"false\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mcollectDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.6.8/envs/ss-spark/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'false'"
     ]
    }
   ],
   "source": [
    "# Collecting Rows to the Driver\n",
    "# collect(), take(), show()\n",
    "# 로컬에서 처리하기 위해서는 driver로 데이터를 수집해야한다.\n",
    "\n",
    "collectDf = df.limit(10)\n",
    "collectDf.take(5)\n",
    "collectDf.show()\n",
    "collectDf.show(5, \"false\")\n",
    "collectDf.collect()\n",
    "\n",
    "collectDf.toLocalIterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-score",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
